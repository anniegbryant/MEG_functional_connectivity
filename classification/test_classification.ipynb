{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/headnode1/abry4213/.conda/envs/pyspi/lib/python3.9/site-packages/sklearn/metrics/_scorer.py:610: FutureWarning: The `needs_threshold` and `needs_proba` parameter are deprecated in version 1.4 and will be removed in 1.6. You can either let `response_method` be `None` or set it to `predict` to preserve the same behaviour.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "from sklearn import svm\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedGroupKFold, cross_validate, StratifiedKFold, LeaveOneOut\n",
    "from sklearn.metrics import make_scorer, roc_auc_score, balanced_accuracy_score, accuracy_score\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from copy import deepcopy\n",
    "import itertools\n",
    "import argparse\n",
    "from os import path as op\n",
    "from glob import glob\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# add path to classification analysis functions\n",
    "from mixed_sigmoid_normalisation import MixedSigmoidScaler\n",
    "\n",
    "\n",
    "bids_root = \"/headnode1/abry4213/data/Cogitate_MEG\"\n",
    "n_jobs = 1\n",
    "subject_ID = \"CA103\"\n",
    "classification_type = \"averaged\"\n",
    "classifier = \"Logistic_Regression\"\n",
    "\n",
    "# Load data paths\n",
    "pyspi_res_path = f\"{bids_root}/derivatives/time_series_features\"\n",
    "pyspi_res_path_averaged = f\"{pyspi_res_path}/averaged_epochs\"\n",
    "pyspi_res_path_individual = f\"{pyspi_res_path}/individual_epochs\"\n",
    "\n",
    "classification_res_path = f\"{bids_root}/derivatives/classification_results\"\n",
    "classification_res_path_averaged = f\"{classification_res_path}/across_participants\"\n",
    "classification_res_path_individual = f\"{classification_res_path}/within_participants\"\n",
    "\n",
    "# Make classification result directories\n",
    "os.makedirs(classification_res_path_averaged, exist_ok=True)\n",
    "os.makedirs(classification_res_path_individual, exist_ok=True)\n",
    "\n",
    "# Define classifier\n",
    "if classifier == \"Linear_SVM\":\n",
    "    model = svm.SVC(C=1, class_weight='balanced', kernel='linear', random_state=127, probability=True)\n",
    "else:\n",
    "    model = LogisticRegression(penalty='l1', C=1, solver='liblinear', class_weight='balanced', random_state=127)\n",
    "\n",
    "pipe = Pipeline([('scaler', MixedSigmoidScaler(unit_variance=True)), \n",
    "                            ('model', model)])\n",
    "\n",
    "# Define scoring type\n",
    "scoring = {'accuracy': 'accuracy',\n",
    "           'balanced_accuracy': 'balanced_accuracy',\n",
    "           'AUC': make_scorer(roc_auc_score, needs_proba=True)}\n",
    "\n",
    "# Defiene cross-validators\n",
    "LOOCV = LeaveOneOut()\n",
    "SKF = StratifiedKFold(n_splits=5, shuffle=True, random_state=127)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in pyspi results\n",
    "all_pyspi_res_list = []\n",
    "for pyspi_res_file in glob(f\"{pyspi_res_path_averaged}/*all_pyspi_results_1000ms.csv\"):\n",
    "    pyspi_res = pd.read_csv(pyspi_res_file)\n",
    "    # Reset index\n",
    "    pyspi_res.reset_index(inplace=True, drop=True)\n",
    "    pyspi_res['stimulus_type'] = pyspi_res['stimulus_type'].replace(False, 'false').replace('False', 'false')\n",
    "    pyspi_res['relevance_type'] = pyspi_res['relevance_type'].replace(\"Relevant non-target\", \"Relevant-non-target\")\n",
    "    # Rename stimulus to stimulus_presentation if it is present\n",
    "    if 'stimulus' in pyspi_res.columns:\n",
    "        if 'stimulus_presentation' in pyspi_res.columns:\n",
    "            pyspi_res.drop(columns=['stimulus'], inplace=True)\n",
    "        else:\n",
    "            pyspi_res = pyspi_res.rename(columns={'stimulus': 'stimulus_presentation'})\n",
    "\n",
    "    all_pyspi_res_list.append(pyspi_res)\n",
    "all_pyspi_res = pd.concat(all_pyspi_res_list)\n",
    "\n",
    "# Define comparisons\n",
    "\n",
    "# meta-ROI comparisons\n",
    "Meta_ROIs = [\"Category_Selective\", \"IPS\", \"Prefrontal_Cortex\", \"V1_V2\"]\n",
    "meta_ROI_comparisons = list(itertools.permutations(Meta_ROIs, 2))\n",
    "\n",
    "# Relevance type comparisons\n",
    "relevance_type_comparisons = [\"Relevant-non-target\", \"Irrelevant\"]\n",
    "\n",
    "# Stimulus presentation comparisons\n",
    "stimulus_presentation_comparisons = [\"on\", \"off\"]\n",
    "\n",
    "# Stimulus type comparisons\n",
    "stimulus_types = all_pyspi_res.stimulus_type.unique().tolist()\n",
    "stimulus_type_comparisons = list(itertools.combinations(stimulus_types, 2))\n",
    "\n",
    "# Also add in face vs. non-face\n",
    "stimulus_type_comparisons.append((\"face\", \"non-face\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_task_classifier(direction, meta_roi_comparison, stimulus_presentation, pyspi_data):\n",
    "    ROI_from, ROI_to = meta_roi_comparison\n",
    "    # Filter pyspi data\n",
    "    pyspi_data = (pyspi_data.query(\"meta_ROI_from == @ROI_from & meta_ROI_to == @ROI_to & stimulus_presentation == @stimulus_presentation\")\n",
    "                    .reset_index(drop=True)\n",
    "                    .drop(columns=['index']))\n",
    "    \n",
    "    # All comparisons list\n",
    "    cross_task_classification_results_list = []\n",
    "\n",
    "    for SPI in pyspi_data.SPI.unique():\n",
    "        # Extract this SPI\n",
    "        this_SPI_data = pyspi_data.query(f\"SPI == '{SPI}'\")\n",
    "\n",
    "        # Find overall number of rows\n",
    "        num_rows = this_SPI_data.shape[0]\n",
    "\n",
    "        # Extract SPI values\n",
    "        this_column_data = this_SPI_data[\"value\"]\n",
    "\n",
    "        # Find number of NaN in this column \n",
    "        num_NaN = this_column_data.isna().sum()\n",
    "        prop_NaN = num_NaN / num_rows\n",
    "\n",
    "        # Find mode and SD\n",
    "        column_mode_max = this_column_data.value_counts().max()\n",
    "        column_SD = this_column_data.std()\n",
    "\n",
    "        # If 0% < num_NaN < 10%, impute by the mean of each component\n",
    "        if 0 < prop_NaN < 0.1:\n",
    "            values_imputed = (this_column_data\n",
    "                                .transform(lambda x: x.fillna(x.mean())))\n",
    "\n",
    "            this_column_data = values_imputed\n",
    "            print(f\"Imputing column values for {SPI}\")\n",
    "            this_SPI_data[\"value\"] = this_column_data\n",
    "\n",
    "        # If there are: \n",
    "        # - more than 10% NaN values;\n",
    "        # - more than 90% of the values are the same; OR\n",
    "        # - the standard deviation is less than 1*10**(-10)\n",
    "        # then remove the column\n",
    "        if prop_NaN > 0.1 or column_mode_max / num_rows > 0.9 or column_SD < 1*10**(-10):\n",
    "            print(f\"{SPI} has low SD: {column_SD}, and/or too many mode occurences: {column_mode_max} out of {num_rows}, and/or {100*prop_NaN}% NaN\")\n",
    "            continue\n",
    "    \n",
    "        # Iterate over stimulus combos\n",
    "        for this_combo in stimulus_type_comparisons:\n",
    "\n",
    "            # Subset data to the corresponding stimulus pairs\n",
    "            if this_combo == (\"face\", \"non-face\"):\n",
    "                final_dataset_for_classification_this_combo = this_SPI_data.assign(stimulus_type = lambda x: np.where(x.stimulus_type == \"face\", \"face\", \"non-face\"))\n",
    "            else:\n",
    "                final_dataset_for_classification_this_combo = this_SPI_data.query(f\"stimulus_type in {this_combo}\")\n",
    "\n",
    "            if direction == \"relevant_to_irrelevant\":\n",
    "                train_df = final_dataset_for_classification_this_combo.query(\"relevance_type == 'Relevant-non-target'\")\n",
    "                test_df = final_dataset_for_classification_this_combo.query(\"relevance_type == 'Irrelevant'\")\n",
    "            else:\n",
    "                train_df = final_dataset_for_classification_this_combo.query(\"relevance_type == 'Irrelevant'\")\n",
    "                test_df = final_dataset_for_classification_this_combo.query(\"relevance_type == 'Relevant-non-target'\")\n",
    "\n",
    "            # Make a deepcopy of the pipeline\n",
    "            this_iter_pipe = deepcopy(pipe)\n",
    "\n",
    "            # Fit classifier\n",
    "            X_train = train_df.value.to_numpy().reshape(-1, 1)\n",
    "            y_train = train_df.stimulus_type.to_numpy().reshape(-1, 1)\n",
    "            X_test = test_df.value.to_numpy().reshape(-1, 1)\n",
    "            y_test = test_df.stimulus_type.to_numpy().reshape(-1, 1)\n",
    "\n",
    "            this_iter_pipe.fit(X_train, y_train)\n",
    "            y_pred = this_iter_pipe.predict(X_test)\n",
    "\n",
    "            # Compute accuracy, balanced accuracy, and AUC\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            balanced_accuracy = balanced_accuracy_score(y_test, y_pred)\n",
    "            \n",
    "            this_SPI_combo_df = pd.DataFrame({\"SPI\": [SPI], \n",
    "                    \"classifier\": [classifier],\n",
    "                    \"meta_ROI_from\": [ROI_from],\n",
    "                    \"meta_ROI_to\": [ROI_to],\n",
    "                    \"cross_task_direction\": [direction],\n",
    "                    \"stimulus_presentation\": [stimulus_presentation],\n",
    "                    \"stimulus_combo\": [this_combo], \n",
    "                    \"accuracy\": [accuracy],\n",
    "                    \"balanced_accuracy\": [balanced_accuracy]})\n",
    "            \n",
    "            # Append to growing results list\n",
    "            cross_task_classification_results_list.append(this_SPI_combo_df)\n",
    "\n",
    "    # Concatenate all results\n",
    "    cross_task_classification_results_df = pd.concat(cross_task_classification_results_list)\n",
    "\n",
    "    # Return results\n",
    "    return cross_task_classification_results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe with all combinations of direction, stimulus_presentation, and meta_roi_comparison\n",
    "all_combos = list(itertools.product([\"relevant_to_irrelevant\", \"irrelevant_to_relevant\"], \n",
    "                                    [\"on\", \"off\"], \n",
    "                                    meta_ROI_comparisons))\n",
    "\n",
    "# Convert to a dataframe\n",
    "all_combos_df = pd.DataFrame(all_combos, columns=[\"cross_task_direction\", \"stimulus_presentation\", \"meta_ROI_comparison\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# direction = \"relevant_to_irrelevant\"\n",
    "# stimulus_presentation = \"on\"\n",
    "# meta_roi_comparison = meta_roi_comparisons[0]\n",
    "# meta_roi_from, meta_roi_to = meta_roi_comparison\n",
    "\n",
    "for direction in [\"relevant_to_irrelevant\", \"irrelevant_to_relevant\"]:\n",
    "    for stimulus_presentation in stimulus_presentation_comparisons:\n",
    "        for meta_roi_comparison in meta_roi_comparisons:\n",
    "            meta_roi_from, meta_roi_to = meta_roi_comparison\n",
    "            example_pyspi_data = all_pyspi_res.query(f\"meta_ROI_from == '{meta_roi_from}' & meta_ROI_to == '{meta_roi_to}' & stimulus_presentation == '{stimulus_presentation}' & SPI in ['cov_EmpiricalCovariance', 'prec_EmpiricalCovariance']\")\n",
    "            trial = cross_task_classifier(direction=direction, \n",
    "                                        meta_roi_comparison=meta_roi_comparison, \n",
    "                                        stimulus_presentation=stimulus_presentation, \n",
    "                                        pyspi_data=example_pyspi_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_jobs = 1\n",
    "test = Parallel(n_jobs=int(n_jobs))(delayed(cross_task_classifier)(direction=direction, \n",
    "                                                                    meta_roi_comparison=meta_roi_comparison, \n",
    "                                                                    stimulus_presentation=stimulus_presentation, \n",
    "                                                                    pyspi_data=all_pyspi_res.query(\"SPI in ['cov_EmpiricalCovariance', 'prec_EmpiricalCovariance']\"))\n",
    "                                                for direction, stimulus_presentation, meta_roi_comparison in all_combos)\n",
    "\n",
    "test_results = pd.concat(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = pd.read_csv(f\"{classification_res_path_averaged}/cross_task_Logistic_Regression_classification_results.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
