{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The rpy2.ipython extension is already loaded. To reload it, use:\n",
      "  %reload_ext rpy2.ipython\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "import itertools\n",
    "\n",
    "# Statistics\n",
    "from tslearn import barycenters\n",
    "from scipy.stats import wasserstein_distance\n",
    "\n",
    "# Classification\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import make_scorer, roc_auc_score, accuracy_score, balanced_accuracy_score\n",
    "from sklearn.model_selection import StratifiedGroupKFold, cross_validate, StratifiedKFold, LeaveOneOut, cross_val_predict\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# add path to classification analysis functions\n",
    "from mixed_sigmoid_normalisation import MixedSigmoidScaler\n",
    "\n",
    "# Add rpy2\n",
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R \n",
    "\n",
    "suppressPackageStartupMessages({\n",
    "    library(broom)\n",
    "    library(cowplot)\n",
    "    library(glue)\n",
    "    library(patchwork)\n",
    "    library(see)\n",
    "    library(tidyverse)\n",
    "})\n",
    "\n",
    "# Set cowplot theme\n",
    "theme_set(theme_cowplot())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pyspi SPI info\n",
    "pyspi_SPI_info = pd.read_csv(\"../feature_extraction/pyspi_SPI_info.csv\")\n",
    "\n",
    "# Define path for derivatives directory\n",
    "deriv_dir = \"/Users/abry4213/data/Cogitate_MEG/derivatives\"\n",
    "\n",
    "# Initialize list for pyspi results\n",
    "all_time_resolved_barycenter_res_list = []\n",
    "\n",
    "# Load in time-resolved barycenter results\n",
    "for barycenter_time_res_file in glob(f\"{deriv_dir}/time_series_features/averaged_epochs/*barycenter*.csv\"):\n",
    "    barycenter_time_resolved_res = pd.read_csv(barycenter_time_res_file)\n",
    "    all_time_resolved_barycenter_res_list.append(barycenter_time_resolved_res)\n",
    "\n",
    "# Concatenate pyspi results\n",
    "all_time_resolved_barycenter_res = pd.concat(all_time_resolved_barycenter_res_list)\n",
    "all_time_resolved_barycenter_res.head()\n",
    "\n",
    "# Separate out into raw and absolute value barycenter by filtering out where 'Region' contains the string 'abs'\n",
    "all_raw_time_resolved_barycenter_res = all_time_resolved_barycenter_res.query(\"Region.str.contains('abs') == False\")\n",
    "all_abs_time_resolved_barycenter_res = all_time_resolved_barycenter_res.query(\"Region.str.contains('abs') == True\").assign(Region=lambda x: x.Region.str.replace(\"_abs\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Presentation</th>\n",
       "      <th>Barycenter_Type</th>\n",
       "      <th>Mean</th>\n",
       "      <th>Max</th>\n",
       "      <th>Max_Time</th>\n",
       "      <th>Subject</th>\n",
       "      <th>Region</th>\n",
       "      <th>Relevance</th>\n",
       "      <th>Stimulus</th>\n",
       "      <th>Barycenter_Method</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Onset</td>\n",
       "      <td>Original</td>\n",
       "      <td>-2.839332e-17</td>\n",
       "      <td>2.314943</td>\n",
       "      <td>563.0</td>\n",
       "      <td>sub-CB049</td>\n",
       "      <td>CS_PFC</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>False</td>\n",
       "      <td>euclidean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Offset</td>\n",
       "      <td>Original</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.280326</td>\n",
       "      <td>785.0</td>\n",
       "      <td>sub-CB049</td>\n",
       "      <td>CS_PFC</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>False</td>\n",
       "      <td>euclidean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Onset</td>\n",
       "      <td>Squared</td>\n",
       "      <td>5.649296e-01</td>\n",
       "      <td>8.072949</td>\n",
       "      <td>972.0</td>\n",
       "      <td>sub-CB049</td>\n",
       "      <td>CS_PFC</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>False</td>\n",
       "      <td>euclidean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Offset</td>\n",
       "      <td>Squared</td>\n",
       "      <td>5.647868e-01</td>\n",
       "      <td>5.199888</td>\n",
       "      <td>785.0</td>\n",
       "      <td>sub-CB049</td>\n",
       "      <td>CS_PFC</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>False</td>\n",
       "      <td>euclidean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Onset</td>\n",
       "      <td>Original</td>\n",
       "      <td>-2.839332e-17</td>\n",
       "      <td>1.565117</td>\n",
       "      <td>770.0</td>\n",
       "      <td>sub-CB049</td>\n",
       "      <td>CS_VIS</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>False</td>\n",
       "      <td>euclidean</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Presentation Barycenter_Type          Mean       Max  Max_Time    Subject  \\\n",
       "0        Onset        Original -2.839332e-17  2.314943     563.0  sub-CB049   \n",
       "1       Offset        Original  0.000000e+00  2.280326     785.0  sub-CB049   \n",
       "2        Onset         Squared  5.649296e-01  8.072949     972.0  sub-CB049   \n",
       "3       Offset         Squared  5.647868e-01  5.199888     785.0  sub-CB049   \n",
       "8        Onset        Original -2.839332e-17  1.565117     770.0  sub-CB049   \n",
       "\n",
       "   Region   Relevance Stimulus Barycenter_Method  \n",
       "0  CS_PFC  Irrelevant    False         euclidean  \n",
       "1  CS_PFC  Irrelevant    False         euclidean  \n",
       "2  CS_PFC  Irrelevant    False         euclidean  \n",
       "3  CS_PFC  Irrelevant    False         euclidean  \n",
       "8  CS_VIS  Irrelevant    False         euclidean  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_raw_time_resolved_barycenter_res.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Robustness check 1: stimulus classification with raw vs. absolute value time series before computing barycenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define classifier\n",
    "model = LogisticRegression(penalty='l1', C=1, solver='liblinear', class_weight='balanced', random_state=127)\n",
    "\n",
    "pipe = Pipeline([('scaler', MixedSigmoidScaler(unit_variance=True)), \n",
    "                            ('model', model)])\n",
    "\n",
    "# Define scoring type\n",
    "scoring = {'accuracy': 'accuracy',\n",
    "           'balanced_accuracy': 'balanced_accuracy',\n",
    "           'AUC': make_scorer(roc_auc_score, response_method='predict_proba')}\n",
    "\n",
    "# meta-ROI comparisons\n",
    "meta_ROIs = [\"Category_Selective\", \"IPS\", \"Prefrontal_Cortex\", \"V1_V2\"]\n",
    "\n",
    "# Manually define combinations\n",
    "meta_roi_comparisons = [(\"Category_Selective\", \"IPS\"),\n",
    "                        (\"Category_Selective\", \"Prefrontal_Cortex\"),\n",
    "                        (\"Category_Selective\", \"V1_V2\"),\n",
    "                        (\"IPS\", \"Category_Selective\"),\n",
    "                        (\"Prefrontal_Cortex\", \"Category_Selective\"),\n",
    "                        (\"V1_V2\", \"Category_Selective\")]\n",
    "\n",
    "# Relevance type comparisons\n",
    "relevance_type_comparisons = [\"Relevant-non-target\", \"Irrelevant\"]\n",
    "\n",
    "# Stimulus presentation comparisons\n",
    "stimulus_presentation_comparisons = [\"on\", \"off\"]\n",
    "\n",
    "# Define all combinations for cross-task classification\n",
    "group_stratified_CV = StratifiedGroupKFold(n_splits = 10, shuffle = True, random_state=127)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stimulus type comparisons\n",
    "stimulus_types = all_raw_time_resolved_barycenter_res.stimulus_type.unique().tolist()\n",
    "stimulus_type_comparisons = list(itertools.combinations(stimulus_types, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All comparisons list\n",
    "comparing_between_stimulus_types_classification_results_list = []\n",
    "\n",
    "for relevance_type in relevance_type_comparisons:\n",
    "    print(\"Relevance type:\" + str(relevance_type))\n",
    "    for stimulus_presentation in stimulus_presentation_comparisons:\n",
    "        print(\"Stimulus presentation:\" + str(stimulus_presentation))\n",
    "        for SPI in all_pyspi_res.SPI.unique():\n",
    "            # First, look at each meta-ROI pair separately\n",
    "            for meta_roi_comparison in meta_roi_comparisons:\n",
    "                print(\"ROI Comparison:\" + str(meta_roi_comparison))\n",
    "                ROI_from, ROI_to = meta_roi_comparison\n",
    "                # Finally, we get to the final dataset\n",
    "                roi_pair_wise_dataset_for_classification = (all_pyspi_res.query(\"meta_ROI_from == @ROI_from & meta_ROI_to == @ROI_to & relevance_type == @relevance_type & stimulus_presentation == @stimulus_presentation\")\n",
    "                                                            .reset_index(drop=True)\n",
    "                                                            .drop(columns=['index']))\n",
    "\n",
    "                # Extract this SPI\n",
    "                this_SPI_data = roi_pair_wise_dataset_for_classification.query(f\"SPI == '{SPI}'\")\n",
    "\n",
    "                # Find overall number of rows\n",
    "                num_rows = this_SPI_data.shape[0]\n",
    "\n",
    "                # Extract SPI values\n",
    "                this_column_data = this_SPI_data[\"value\"]\n",
    "\n",
    "                # Find number of NaN in this column \n",
    "                num_NaN = this_column_data.isna().sum()\n",
    "                prop_NaN = num_NaN / num_rows\n",
    "\n",
    "                # Find mode and SD\n",
    "                column_mode_max = this_column_data.value_counts().max()\n",
    "                column_SD = this_column_data.std()\n",
    "\n",
    "                # If 0% < num_NaN < 10%, impute by the mean of each component\n",
    "                if 0 < prop_NaN < 0.1:\n",
    "                    values_imputed = (this_column_data\n",
    "                                        .transform(lambda x: x.fillna(x.mean())))\n",
    "\n",
    "                    this_column_data = values_imputed\n",
    "                    print(f\"Imputing column values for {SPI}\")\n",
    "                    this_SPI_data[\"value\"] = this_column_data\n",
    "\n",
    "                # If there are: \n",
    "                # - more than 10% NaN values;\n",
    "                # - more than 90% of the values are the same; OR\n",
    "                # - the standard deviation is less than 1*10**(-10)\n",
    "                # then remove the column\n",
    "                if prop_NaN > 0.1 or column_mode_max / num_rows > 0.9 or column_SD < 1*10**(-10):\n",
    "                    print(f\"{SPI} has low SD: {column_SD}, and/or too many mode occurences: {column_mode_max} out of {num_rows}, and/or {100*prop_NaN}% NaN\")\n",
    "                    continue\n",
    "                \n",
    "                # Start an empty list for the classification results\n",
    "                SPI_combo_res_list = []\n",
    "            \n",
    "                # Iterate over stimulus combos\n",
    "                for this_combo in stimulus_type_comparisons:\n",
    "\n",
    "                    # Subset data to the corresponding stimulus pairs\n",
    "                    final_dataset_for_classification_this_combo = this_SPI_data.query(f\"stimulus_type in {this_combo}\")\n",
    "\n",
    "                    # Fit classifier\n",
    "                    X = final_dataset_for_classification_this_combo.value.to_numpy().reshape(-1, 1)\n",
    "                    y = final_dataset_for_classification_this_combo.stimulus_type.to_numpy().reshape(-1, 1)\n",
    "                    groups = final_dataset_for_classification_this_combo.subject_ID.to_numpy().reshape(-1, 1)\n",
    "                    groups_flat = np.array([str(item[0]) for item in groups])\n",
    "\n",
    "\n",
    "                    # Make a deepcopy of the pipeline\n",
    "                    this_iter_pipe = deepcopy(pipe)\n",
    "                    this_classifier_res = cross_validate(this_iter_pipe, X, y, groups=groups_flat, cv=group_stratified_CV, scoring=scoring, n_jobs=n_jobs, \n",
    "                                                                return_estimator=False, return_train_score=False)\n",
    "                    \n",
    "                    this_SPI_combo_df = pd.DataFrame({\"SPI\": [SPI], \n",
    "                            \"classifier\": [classifier],\n",
    "                            \"meta_ROI_from\": [ROI_from],\n",
    "                            \"meta_ROI_to\": [ROI_to],\n",
    "                            \"relevance_type\": [relevance_type],\n",
    "                            \"stimulus_presentation\": [stimulus_presentation],\n",
    "                            \"stimulus_combo\": [this_combo], \n",
    "                            \"accuracy\": [this_classifier_res['test_accuracy'].mean()],\n",
    "                            \"accuracy_SD\": [this_classifier_res['test_accuracy'].std()]})\n",
    "                    \n",
    "                    # Append to growing results list\n",
    "                    comparing_between_stimulus_types_classification_results_list.append(this_SPI_combo_df)\n",
    "\n",
    "comparing_between_stimulus_types_classification_results = pd.concat(comparing_between_stimulus_types_classification_results_list).reset_index(drop=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "annie_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
